# phrases_extractor

一个从 **自然语言文本**  中抽取 **关键短语** 的工具

## 应用场景

- 在很多关键词提取任务中，使用tfidf、textrank等方法提取得到的仅仅是若干零碎词汇。
- 这样的零碎词汇无法真正的表达文章的原本含义，我们并不想要它。  
例如：
```
>>> text = '朝鲜确认金正恩出访俄罗斯 将与普京举行会谈...'
>>> keywords = ['俄罗斯', '朝鲜', '普京', '金正恩', '俄方']
```

- 在很多时候，我们往往需要更细化的短语描述，来作为文本的关键信息展示。这样的需求在生成词云、提供摘要阅读、关键信息检索等任务中都非常重要。  
例如：
```
>>> phrases = ['俄罗斯克里姆林宫', '邀请金正恩访俄', '最高司令官金正恩', 
               '朝方转交普京', '举行会谈']
```

## 功能介绍

为解决以上问题，基于北大分词器 pkuseg工具 工具，开发了一个关键短语抽取器，它可以方便地从文本中找出表达完成意思的关键短语。


## 使用方法

#### 安装
- 仅支持 python3
- 自动安装 pkuseg 依赖包

```
$ git clone https://github.com/dongrixinyu/phrases_extractor
$ cd ~/phrases_extractor
$ pip install .
```


#### 调用

- 输入必须为 **utf-8** 编码字符串
- 具体函数参数见代码
```
import phrases_extractor as pe  
# 初次导入时会自动下载北大词性标注模型包
# 若由于网速原因下载失败，请参考 https://github.com/lancopku/pkuseg-python 如何安装下载 pkuseg 默认模型

text = '法国媒体最新披露，巴黎圣母院火灾当晚，第一次消防警报响起时，负责查验的保安找错了位置，因而可能贻误了救火的最佳时机。...'
phrases = pe.extract_phrases(text)
print(phrases)
```

## 新版 2.0
- 从 jieba 分词器迁移到 pkuseg，因为 jieba 分词器过于粗放
- 新增了停用词规则、虚词规则等


## 原理

- 首先基于 pkuseg 工具做分词和词性标注，再使用 tfidf 计算文本的关键词权重， 
- 关键词提取算法找出碎片化的关键词，然后再根据相邻关键碎片词进行融合，重新计算权重，去除相似词汇。得到的融合的多个关键碎片即为关键短语。
    - 短语的 token 长度不超过 20
    - 短语中不可出现超过1个虚词
    - 短语的两端 token 不可是虚词停用词
    - 短语中不可以超过规定个数停用词
    - 短语重复度计算添加其中

## 我的窝

如果觉得方便好用，请 follow 我一波：https://github.com/dongrixinyu


